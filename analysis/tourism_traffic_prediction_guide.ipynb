{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af4be5e",
   "metadata": {},
   "source": [
    "# üîó B∆Ø·ªöC 0: GH√âP D·ªÆ LI·ªÜU (DATA MERGING)\n",
    "\n",
    "## T·∫°i sao c·∫ßn gh√©p d·ªØ li·ªáu?\n",
    "\n",
    "| File | Key li√™n k·∫øt | C·∫•u tr√∫c |\n",
    "|------|-------------|----------|\n",
    "| `vietnam_destinations_normalized.csv` | date | 180 rows √ó 967 columns (wide format) |\n",
    "| `vietnam_weather_monthly_2011_2025.csv` | province + date | 6,078 rows (long format) |\n",
    "| `keyword_mapping_normalized.csv` | destination ‚Üí province | Mapping table |\n",
    "\n",
    "**V·∫•n ƒë·ªÅ**: Traffic data theo destination, Weather data theo province ‚Üí C·∫ßn mapping ƒë·ªÉ gh√©p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2cc5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading all data files...\n",
      "\n",
      "‚úÖ Traffic data: (180, 969) (rows=months, cols=destinations)\n",
      "‚úÖ Weather data: (6076, 6)\n",
      "‚úÖ Mapping data: (967, 6)\n",
      "‚úÖ Festivals data: (142, 13)\n",
      "‚úÖ Seasonal patterns: (117, 11)\n",
      "‚úÖ Statistics: (968, 8)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.1: LOAD T·∫§T C·∫¢ D·ªÆ LI·ªÜU\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üìÇ Loading all data files...\")\n",
    "\n",
    "# Main traffic data (wide format: rows=months, cols=destinations)\n",
    "traffic_df = pd.read_csv('../data/normalized/vietnam_destinations_normalized.csv')\n",
    "\n",
    "# Weather data (long format: rows=province+month)\n",
    "weather_df = pd.read_csv('../data/normalized/vietnam_weather_monthly_2011_2025.csv')\n",
    "\n",
    "# Mapping destination ‚Üí province\n",
    "mapping_df = pd.read_csv('../data/normalized/keyword_mapping_normalized.csv')\n",
    "\n",
    "# Festivals data\n",
    "festivals_df = pd.read_csv('../data/normalized/vietnam_festivals.csv')\n",
    "\n",
    "# Seasonal patterns (117 destinations v·ªõi strong seasonality)\n",
    "seasonal_df = pd.read_csv('../data/normalized/vietnam_seasonal_destinations_strong.csv')\n",
    "\n",
    "# Destinations statistics\n",
    "stats_df = pd.read_csv('../data/normalized/destinations_statistics.csv')\n",
    "\n",
    "print(f\"\\n‚úÖ Traffic data: {traffic_df.shape} (rows=months, cols=destinations)\")\n",
    "print(f\"‚úÖ Weather data: {weather_df.shape}\")\n",
    "print(f\"‚úÖ Mapping data: {mapping_df.shape}\")\n",
    "print(f\"‚úÖ Festivals data: {festivals_df.shape}\")\n",
    "print(f\"‚úÖ Seasonal patterns: {seasonal_df.shape}\")\n",
    "print(f\"‚úÖ Statistics: {stats_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd6343bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Date range: 2011-01-01 00:00:00 to 2025-12-01 00:00:00\n",
      "   Total months: 180\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.2: CHUY·ªÇN ƒê·ªîI DATE FORMAT\n",
    "# =============================================================================\n",
    "\n",
    "def parse_vietnamese_date(date_str):\n",
    "    \"\"\"Chuy·ªÉn 'thg 1 2011' ‚Üí datetime\"\"\"\n",
    "    months_vi = {\n",
    "        'thg 1': 1, 'thg 2': 2, 'thg 3': 3, 'thg 4': 4,\n",
    "        'thg 5': 5, 'thg 6': 6, 'thg 7': 7, 'thg 8': 8,\n",
    "        'thg 9': 9, 'thg 10': 10, 'thg 11': 11, 'thg 12': 12\n",
    "    }\n",
    "    parts = date_str.rsplit(' ', 1)  # Split from right: ['thg 1', '2011']\n",
    "    month_str, year = parts[0], int(parts[1])\n",
    "    month = months_vi.get(month_str, 1)\n",
    "    return pd.Timestamp(year=year, month=month, day=1)\n",
    "\n",
    "# Convert traffic date column\n",
    "traffic_df['date_parsed'] = traffic_df['date'].apply(parse_vietnamese_date)\n",
    "print(f\"üìÖ Date range: {traffic_df['date_parsed'].min()} to {traffic_df['date_parsed'].max()}\")\n",
    "print(f\"   Total months: {len(traffic_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "724bf3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Number of destinations: 968\n",
      "\n",
      "üìä Traffic data after melt:\n",
      "   Shape: (174240, 3)\n",
      "   Columns: ['date_parsed', 'destination', 'traffic']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_parsed</th>\n",
       "      <th>destination</th>\n",
       "      <th>traffic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>Ch·ª£ Trung t√¢m Ba Ch·∫Ω</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>Ch·ª£ Trung t√¢m Ba Ch·∫Ω</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Ch·ª£ Trung t√¢m Ba Ch·∫Ω</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>Ch·ª£ Trung t√¢m Ba Ch·∫Ω</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Ch·ª£ Trung t√¢m Ba Ch·∫Ω</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  date_parsed           destination  traffic\n",
       "0  2011-01-01  Ch·ª£ Trung t√¢m Ba Ch·∫Ω      0.0\n",
       "1  2012-01-01  Ch·ª£ Trung t√¢m Ba Ch·∫Ω      0.0\n",
       "2  2013-01-01  Ch·ª£ Trung t√¢m Ba Ch·∫Ω      0.0\n",
       "3  2014-01-01  Ch·ª£ Trung t√¢m Ba Ch·∫Ω      0.0\n",
       "4  2015-01-01  Ch·ª£ Trung t√¢m Ba Ch·∫Ω      0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.3: CHUY·ªÇN TRAFFIC DATA T·ª™ WIDE ‚Üí LONG FORMAT\n",
    "# =============================================================================\n",
    "# Wide format: rows=months, cols=destinations\n",
    "# Long format: rows=month√ódestination (ƒë·ªÉ d·ªÖ merge v·ªõi weather)\n",
    "\n",
    "# Get destination columns (all except 'date' and 'date_parsed')\n",
    "destination_cols = [col for col in traffic_df.columns if col not in ['date', 'date_parsed']]\n",
    "print(f\"üìä Number of destinations: {len(destination_cols)}\")\n",
    "\n",
    "# Melt to long format\n",
    "traffic_long = traffic_df.melt(\n",
    "    id_vars=['date_parsed'],\n",
    "    value_vars=destination_cols,\n",
    "    var_name='destination',\n",
    "    value_name='traffic'\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Traffic data after melt:\")\n",
    "print(f\"   Shape: {traffic_long.shape}\")\n",
    "print(f\"   Columns: {traffic_long.columns.tolist()}\")\n",
    "traffic_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e73386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Destinations without province mapping: 1\n",
      "   Examples: ['Rau m√°']\n",
      "\n",
      "‚úÖ Traffic data after mapping: (174060, 4)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.4: GH√âP MAPPING (destination ‚Üí province)\n",
    "# =============================================================================\n",
    "\n",
    "# T·∫°o mapping dict t·ª´ normalized_name ‚Üí province_normalized\n",
    "dest_to_province = dict(zip(\n",
    "    mapping_df['normalized_name'], \n",
    "    mapping_df['province_normalized']\n",
    "))\n",
    "\n",
    "# Map province v√†o traffic_long\n",
    "traffic_long['province'] = traffic_long['destination'].map(dest_to_province)\n",
    "\n",
    "# Ki·ªÉm tra missing mappings\n",
    "missing_mapping = traffic_long[traffic_long['province'].isna()]['destination'].unique()\n",
    "print(f\"‚ö†Ô∏è Destinations without province mapping: {len(missing_mapping)}\")\n",
    "if len(missing_mapping) > 0:\n",
    "    print(f\"   Examples: {missing_mapping[:5]}\")\n",
    "\n",
    "# Lo·∫°i b·ªè rows kh√¥ng c√≥ province mapping\n",
    "traffic_long_clean = traffic_long.dropna(subset=['province'])\n",
    "print(f\"\\n‚úÖ Traffic data after mapping: {traffic_long_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb6acf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Merged data shape: (174060, 6)\n",
      "   Missing weather data: 1573 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_parsed</th>\n",
       "      <th>destination</th>\n",
       "      <th>traffic</th>\n",
       "      <th>province</th>\n",
       "      <th>temp_avg</th>\n",
       "      <th>rainfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>Ch·ª£ Trung t√¢m Ba Ch·∫Ω</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Qu·∫£ng Ninh</td>\n",
       "      <td>12.106452</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>Ch·ª£ Trung t√¢m Ba Ch·∫Ω</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Qu·∫£ng Ninh</td>\n",
       "      <td>14.035484</td>\n",
       "      <td>42.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Ch·ª£ Trung t√¢m Ba Ch·∫Ω</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Qu·∫£ng Ninh</td>\n",
       "      <td>14.680645</td>\n",
       "      <td>14.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>Ch·ª£ Trung t√¢m Ba Ch·∫Ω</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Qu·∫£ng Ninh</td>\n",
       "      <td>15.161290</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Ch·ª£ Trung t√¢m Ba Ch·∫Ω</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Qu·∫£ng Ninh</td>\n",
       "      <td>15.532258</td>\n",
       "      <td>36.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>Ch·ª£ Trung t√¢m Ba Ch·∫Ω</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Qu·∫£ng Ninh</td>\n",
       "      <td>15.670968</td>\n",
       "      <td>116.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Ch·ª£ Trung t√¢m Ba Ch·∫Ω</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Qu·∫£ng Ninh</td>\n",
       "      <td>16.587097</td>\n",
       "      <td>41.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Ch·ª£ Trung t√¢m Ba Ch·∫Ω</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Qu·∫£ng Ninh</td>\n",
       "      <td>14.332258</td>\n",
       "      <td>34.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Ch·ª£ Trung t√¢m Ba Ch·∫Ω</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Qu·∫£ng Ninh</td>\n",
       "      <td>15.025806</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>Ch·ª£ Trung t√¢m Ba Ch·∫Ω</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Qu·∫£ng Ninh</td>\n",
       "      <td>16.600000</td>\n",
       "      <td>154.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  date_parsed           destination  traffic    province   temp_avg  rainfall\n",
       "0  2011-01-01  Ch·ª£ Trung t√¢m Ba Ch·∫Ω      0.0  Qu·∫£ng Ninh  12.106452       4.4\n",
       "1  2012-01-01  Ch·ª£ Trung t√¢m Ba Ch·∫Ω      0.0  Qu·∫£ng Ninh  14.035484      42.1\n",
       "2  2013-01-01  Ch·ª£ Trung t√¢m Ba Ch·∫Ω      0.0  Qu·∫£ng Ninh  14.680645      14.3\n",
       "3  2014-01-01  Ch·ª£ Trung t√¢m Ba Ch·∫Ω      0.0  Qu·∫£ng Ninh  15.161290       3.7\n",
       "4  2015-01-01  Ch·ª£ Trung t√¢m Ba Ch·∫Ω      0.0  Qu·∫£ng Ninh  15.532258      36.5\n",
       "5  2016-01-01  Ch·ª£ Trung t√¢m Ba Ch·∫Ω      0.0  Qu·∫£ng Ninh  15.670968     116.7\n",
       "6  2017-01-01  Ch·ª£ Trung t√¢m Ba Ch·∫Ω      0.0  Qu·∫£ng Ninh  16.587097      41.6\n",
       "7  2018-01-01  Ch·ª£ Trung t√¢m Ba Ch·∫Ω      0.0  Qu·∫£ng Ninh  14.332258      34.4\n",
       "8  2019-01-01  Ch·ª£ Trung t√¢m Ba Ch·∫Ω      0.0  Qu·∫£ng Ninh  15.025806      33.4\n",
       "9  2020-01-01  Ch·ª£ Trung t√¢m Ba Ch·∫Ω      0.0  Qu·∫£ng Ninh  16.600000     154.2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.5: GH√âP WEATHER DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Convert weather date to same format\n",
    "weather_df['date_parsed'] = pd.to_datetime(weather_df['date'])\n",
    "\n",
    "# Merge traffic v·ªõi weather theo province + date\n",
    "merged_df = traffic_long_clean.merge(\n",
    "    weather_df[['province', 'temp_avg', 'rainfall', 'date_parsed']],\n",
    "    left_on=['province', 'date_parsed'],\n",
    "    right_on=['province', 'date_parsed'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Merged data shape: {merged_df.shape}\")\n",
    "print(f\"   Missing weather data: {merged_df['temp_avg'].isna().sum()} rows\")\n",
    "\n",
    "# Check sample\n",
    "merged_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6caeac64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seasonal columns: ['Destination', 'Peak_Months', 'Strong_Months (>=1.2x)', 'Primary_Peak_Month', 'Amplitude (Median Peak/Trough)', 'CV (std/median of months)', 'Coverage OK Months', 'C√≥ m√πa v·ª•?', 'Peak_Months_List', 'Num_Strong_Months', 'Seasonal_Months_Clustered']\n",
      "‚úÖ Destinations with strong seasonality: 12.1%\n",
      "\n",
      "üìä Final merged dataset:\n",
      "   Shape: (174060, 13)\n",
      "   Columns: ['date_parsed', 'destination', 'traffic', 'province', 'temp_avg', 'rainfall', 'amplitude', 'Peak_Months', 'Primary_Peak_Month', 'has_strong_seasonality', 'year', 'month', 'quarter']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.6: TH√äM TH√îNG TIN SEASONALITY\n",
    "# =============================================================================\n",
    "\n",
    "# Column names in seasonal_df are different - check and use correct names\n",
    "print(f\"Seasonal columns: {seasonal_df.columns.tolist()}\")\n",
    "\n",
    "# Merge v·ªõi seasonal patterns (using correct column names)\n",
    "seasonal_info = seasonal_df[['Destination', 'Amplitude (Median Peak/Trough)', 'Peak_Months', 'Primary_Peak_Month']]\n",
    "seasonal_info = seasonal_info.rename(columns={\n",
    "    'Destination': 'destination',\n",
    "    'Amplitude (Median Peak/Trough)': 'amplitude'\n",
    "})\n",
    "\n",
    "merged_df = merged_df.merge(seasonal_info, on='destination', how='left')\n",
    "\n",
    "# Mark if destination has strong seasonality\n",
    "merged_df['has_strong_seasonality'] = merged_df['amplitude'].notna()\n",
    "\n",
    "print(f\"‚úÖ Destinations with strong seasonality: {merged_df['has_strong_seasonality'].sum() / len(merged_df) * 100:.1f}%\")\n",
    "\n",
    "# Add time features\n",
    "merged_df['year'] = merged_df['date_parsed'].dt.year\n",
    "merged_df['month'] = merged_df['date_parsed'].dt.month\n",
    "merged_df['quarter'] = merged_df['date_parsed'].dt.quarter\n",
    "\n",
    "print(f\"\\nüìä Final merged dataset:\")\n",
    "print(f\"   Shape: {merged_df.shape}\")\n",
    "print(f\"   Columns: {merged_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfc20644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved merged data to: ../data/normalized/merged_tourism_data.csv\n",
      "\n",
      "============================================================\n",
      "üìã T√ìM T·∫ÆT D·ªÆ LI·ªÜU ƒê√É GH√âP\n",
      "============================================================\n",
      "üìä Total rows: 174,060\n",
      "üìç Unique destinations: 967\n",
      "üó∫Ô∏è Unique provinces: 34\n",
      "üìÖ Date range: 2011-01-01 00:00:00 to 2025-12-01 00:00:00\n",
      "üå°Ô∏è Weather features: temp_avg, rainfall\n",
      "üìà Seasonality info: Amplitude, Peak_Months, Primary_Peak_Month\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_parsed</th>\n",
       "      <th>destination</th>\n",
       "      <th>traffic</th>\n",
       "      <th>province</th>\n",
       "      <th>temp_avg</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>amplitude</th>\n",
       "      <th>Peak_Months</th>\n",
       "      <th>Primary_Peak_Month</th>\n",
       "      <th>has_strong_seasonality</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134833</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>B√°n ƒë·∫£o ƒê·∫ßm M√¥n</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Kh√°nh Ho√†</td>\n",
       "      <td>23.332258</td>\n",
       "      <td>11.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100727</th>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>ƒê√¨nh N·∫°i Nam</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>TP. ƒê√† N·∫µng</td>\n",
       "      <td>28.945161</td>\n",
       "      <td>81.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2013</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85881</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>H·ªì Tuy·ªÅn L√¢m</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>L√¢m ƒê·ªìng</td>\n",
       "      <td>22.112903</td>\n",
       "      <td>231.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77415</th>\n",
       "      <td>2011-10-01</td>\n",
       "      <td>H√≤n M√¢y R√∫t</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>An Giang</td>\n",
       "      <td>26.719355</td>\n",
       "      <td>194.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2011</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>ƒê·ªÅn C√°i L√¢n</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Qu·∫£ng Ninh</td>\n",
       "      <td>21.386667</td>\n",
       "      <td>85.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date_parsed      destination    traffic     province   temp_avg  \\\n",
       "134833  2024-01-01  B√°n ƒë·∫£o ƒê·∫ßm M√¥n   0.000000    Kh√°nh Ho√†  23.332258   \n",
       "100727  2013-05-01     ƒê√¨nh N·∫°i Nam   0.000000  TP. ƒê√† N·∫µng  28.945161   \n",
       "85881   2017-10-01     H·ªì Tuy·ªÅn L√¢m  11.111111     L√¢m ƒê·ªìng  22.112903   \n",
       "77415   2011-10-01      H√≤n M√¢y R√∫t  25.000000     An Giang  26.719355   \n",
       "10001   2022-04-01      ƒê·ªÅn C√°i L√¢n   0.000000   Qu·∫£ng Ninh  21.386667   \n",
       "\n",
       "        rainfall  amplitude Peak_Months  Primary_Peak_Month  \\\n",
       "134833      11.6        NaN         NaN                 NaN   \n",
       "100727      81.0        NaN         NaN                 NaN   \n",
       "85881      231.1        NaN         NaN                 NaN   \n",
       "77415      194.7        NaN         NaN                 NaN   \n",
       "10001       85.6        NaN         NaN                 NaN   \n",
       "\n",
       "        has_strong_seasonality  year  month  quarter  \n",
       "134833                   False  2024      1        1  \n",
       "100727                   False  2013      5        2  \n",
       "85881                    False  2017     10        4  \n",
       "77415                    False  2011     10        4  \n",
       "10001                    False  2022      4        2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.7: L∆ØU D·ªÆ LI·ªÜU ƒê√É GH√âP\n",
    "# =============================================================================\n",
    "\n",
    "# L∆∞u merged dataset\n",
    "output_path = '../data/normalized/merged_tourism_data.csv'\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "print(f\"üíæ Saved merged data to: {output_path}\")\n",
    "\n",
    "# T√≥m t·∫Øt d·ªØ li·ªáu\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"üìã T√ìM T·∫ÆT D·ªÆ LI·ªÜU ƒê√É GH√âP\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìä Total rows: {len(merged_df):,}\")\n",
    "print(f\"üìç Unique destinations: {merged_df['destination'].nunique()}\")\n",
    "print(f\"üó∫Ô∏è Unique provinces: {merged_df['province'].nunique()}\")\n",
    "print(f\"üìÖ Date range: {merged_df['date_parsed'].min()} to {merged_df['date_parsed'].max()}\")\n",
    "print(f\"üå°Ô∏è Weather features: temp_avg, rainfall\")\n",
    "print(f\"üìà Seasonality info: Amplitude, Peak_Months, Primary_Peak_Month\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show sample\n",
    "merged_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9eb3c3",
   "metadata": {},
   "source": [
    "## üîÑ GH√âP TH√äM D·ªÆ LI·ªÜU B·ªî SUNG\n",
    "\n",
    "C√°c file c√≤n l·∫°i c·∫ßn gh√©p:\n",
    "- **Economic**: GRDP, Population, Area\n",
    "- **Infrastructure**: Accommodation, Restaurant, Entertainment, Healthcare, Shop counts\n",
    "- **Geographic**: Regions, Distances\n",
    "- **Social Media**: YouTube views/likes\n",
    "- **Statistics**: Destination statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2fcd2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded additional data files:\n",
      "   GRDP: (441, 4)\n",
      "   Area/Population: (483, 6)\n",
      "   Regions: (34, 10)\n",
      "   YouTube: (36, 5)\n",
      "   Accommodation: (11795, 3)\n",
      "   Restaurant: (2151, 3)\n",
      "   Entertainment: (339, 3)\n",
      "   Healthcare: (329, 3)\n",
      "   Shops: (597, 3)\n",
      "   Dest Stats: (968, 8)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.8: LOAD T·∫§T C·∫¢ D·ªÆ LI·ªÜU B·ªî SUNG\n",
    "# =============================================================================\n",
    "\n",
    "# Economic data\n",
    "grdp_df = pd.read_csv('../data/normalized/vietnam_grdp_by_province.csv')\n",
    "area_pop_df = pd.read_csv('../data/normalized/vietnam_area_population.csv')\n",
    "\n",
    "# Geographic data  \n",
    "regions_df = pd.read_csv('../data/normalized/vietnam_regions_with_distances.csv')\n",
    "\n",
    "# Social media\n",
    "youtube_df = pd.read_csv('../data/normalized/vietnam_youtube_province_aggregates.csv')\n",
    "\n",
    "# Infrastructure counts (need to aggregate by province)\n",
    "accommodation_df = pd.read_csv('../data/normalized/vietnam_accommodation.csv', sep=';')\n",
    "restaurant_df = pd.read_csv('../data/normalized/vietnam_restaurants.csv', sep=';')\n",
    "entertainment_df = pd.read_csv('../data/normalized/vietnam_entertainment.csv', sep=';')\n",
    "healthcare_df = pd.read_csv('../data/normalized/vietnam_healthcare.csv', sep=';')\n",
    "shops_df = pd.read_csv('../data/normalized/vietnam_shops.csv', sep=';')\n",
    "\n",
    "# Destination statistics\n",
    "dest_stats_df = pd.read_csv('../data/normalized/destinations_statistics.csv')\n",
    "\n",
    "print(\"üìÇ Loaded additional data files:\")\n",
    "print(f\"   GRDP: {grdp_df.shape}\")\n",
    "print(f\"   Area/Population: {area_pop_df.shape}\")\n",
    "print(f\"   Regions: {regions_df.shape}\")\n",
    "print(f\"   YouTube: {youtube_df.shape}\")\n",
    "print(f\"   Accommodation: {accommodation_df.shape}\")\n",
    "print(f\"   Restaurant: {restaurant_df.shape}\")\n",
    "print(f\"   Entertainment: {entertainment_df.shape}\")\n",
    "print(f\"   Healthcare: {healthcare_df.shape}\")\n",
    "print(f\"   Shops: {shops_df.shape}\")\n",
    "print(f\"   Dest Stats: {dest_stats_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d747d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Infrastructure counts by province: (33, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>province</th>\n",
       "      <th>accommodation_count</th>\n",
       "      <th>restaurant_count</th>\n",
       "      <th>entertainment_count</th>\n",
       "      <th>healthcare_count</th>\n",
       "      <th>shop_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An Giang</td>\n",
       "      <td>235</td>\n",
       "      <td>53</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B·∫Øc Ninh</td>\n",
       "      <td>132</td>\n",
       "      <td>35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cao B·∫±ng</td>\n",
       "      <td>164</td>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C√† Mau</td>\n",
       "      <td>113</td>\n",
       "      <td>63</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gia Lai</td>\n",
       "      <td>187</td>\n",
       "      <td>36</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   province  accommodation_count  restaurant_count  entertainment_count  \\\n",
       "0  An Giang                  235                53                 12.0   \n",
       "1  B·∫Øc Ninh                  132                35                  0.0   \n",
       "2  Cao B·∫±ng                  164                31                  0.0   \n",
       "3    C√† Mau                  113                63                 15.0   \n",
       "4   Gia Lai                  187                36                  6.0   \n",
       "\n",
       "   healthcare_count  shop_count  \n",
       "0               8.0          13  \n",
       "1               0.0          12  \n",
       "2               0.0           4  \n",
       "3              14.0          14  \n",
       "4               6.0          12  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.9: T·∫†O INFRASTRUCTURE COUNTS BY PROVINCE\n",
    "# =============================================================================\n",
    "\n",
    "# Count infrastructure by province\n",
    "infra_counts = pd.DataFrame()\n",
    "\n",
    "# Accommodation count\n",
    "acc_count = accommodation_df.groupby('province_normalized').size().reset_index(name='accommodation_count')\n",
    "\n",
    "# Restaurant count\n",
    "rest_count = restaurant_df.groupby('province_normalized').size().reset_index(name='restaurant_count')\n",
    "\n",
    "# Entertainment count\n",
    "ent_count = entertainment_df.groupby('province_normalized').size().reset_index(name='entertainment_count')\n",
    "\n",
    "# Healthcare count\n",
    "health_count = healthcare_df.groupby('province_normalized').size().reset_index(name='healthcare_count')\n",
    "\n",
    "# Shop count\n",
    "shop_count = shops_df.groupby('province_normalized').size().reset_index(name='shop_count')\n",
    "\n",
    "# Merge all counts\n",
    "infra_counts = acc_count.merge(rest_count, on='province_normalized', how='outer')\n",
    "infra_counts = infra_counts.merge(ent_count, on='province_normalized', how='outer')\n",
    "infra_counts = infra_counts.merge(health_count, on='province_normalized', how='outer')\n",
    "infra_counts = infra_counts.merge(shop_count, on='province_normalized', how='outer')\n",
    "infra_counts = infra_counts.fillna(0)\n",
    "\n",
    "# Rename column\n",
    "infra_counts = infra_counts.rename(columns={'province_normalized': 'province'})\n",
    "\n",
    "print(f\"üìä Infrastructure counts by province: {infra_counts.shape}\")\n",
    "infra_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abeec899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ After merging regions: (174060, 18)\n",
      "   Columns: ['date_parsed', 'destination', 'traffic', 'province', 'temp_avg', 'rainfall', 'amplitude', 'Peak_Months', 'Primary_Peak_Month', 'has_strong_seasonality', 'year', 'month', 'quarter', 'region', 'latitude', 'longitude', 'distance_to_hanoi_km', 'distance_to_hcm_km']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.10: GH√âP REGIONS & GEOGRAPHIC DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Prepare regions data (static by province)\n",
    "regions_clean = regions_df[['province_normalized', 'region', 'latitude', 'longitude', \n",
    "                            'distance_to_hanoi_km', 'distance_to_hcm_km']].copy()\n",
    "regions_clean = regions_clean.rename(columns={'province_normalized': 'province'})\n",
    "\n",
    "# Merge regions into main dataset\n",
    "merged_df = merged_df.merge(regions_clean, on='province', how='left')\n",
    "\n",
    "print(f\"‚úÖ After merging regions: {merged_df.shape}\")\n",
    "print(f\"   Columns: {merged_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f61903ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ After merging infrastructure: (174060, 23)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.11: GH√âP INFRASTRUCTURE COUNTS\n",
    "# =============================================================================\n",
    "\n",
    "# Merge infrastructure counts into main dataset\n",
    "merged_df = merged_df.merge(infra_counts, on='province', how='left')\n",
    "\n",
    "# Fill NaN with 0 for infrastructure counts\n",
    "infra_cols = ['accommodation_count', 'restaurant_count', 'entertainment_count', \n",
    "              'healthcare_count', 'shop_count']\n",
    "merged_df[infra_cols] = merged_df[infra_cols].fillna(0)\n",
    "\n",
    "print(f\"‚úÖ After merging infrastructure: {merged_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ba6b183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ After merging YouTube: (189720, 26)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.12: GH√âP YOUTUBE DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Prepare YouTube data\n",
    "youtube_clean = youtube_df[['province_normalized', 'views', 'likes', 'comments']].copy()\n",
    "youtube_clean = youtube_clean.rename(columns={\n",
    "    'province_normalized': 'province',\n",
    "    'views': 'youtube_views',\n",
    "    'likes': 'youtube_likes',\n",
    "    'comments': 'youtube_comments'\n",
    "})\n",
    "\n",
    "# Merge YouTube data\n",
    "merged_df = merged_df.merge(youtube_clean, on='province', how='left')\n",
    "\n",
    "print(f\"‚úÖ After merging YouTube: {merged_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdbe5a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRDP columns: ['NƒÉm', 'T√™n t·ªânh, th√†nh ph·ªë', 'T·ªïng GRDP\\n\\xa0(t·ª∑ ƒë·ªìng)', 'province_normalized']\n",
      "‚úÖ After merging GRDP: (278256, 27)\n",
      "   GRDP coverage: 63.6%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.13: GH√âP GRDP DATA (theo province + year)\n",
    "# =============================================================================\n",
    "\n",
    "# Check actual column names\n",
    "print(f\"GRDP columns: {grdp_df.columns.tolist()}\")\n",
    "\n",
    "# Clean GRDP data - find the GRDP column\n",
    "grdp_col = [col for col in grdp_df.columns if 'GRDP' in col][0]\n",
    "grdp_clean = grdp_df[['NƒÉm', 'province_normalized', grdp_col]].copy()\n",
    "grdp_clean.columns = ['year', 'province', 'grdp']\n",
    "\n",
    "# Clean GRDP values (remove dots and commas, convert to float)\n",
    "grdp_clean['grdp'] = grdp_clean['grdp'].astype(str).str.replace('.', '', regex=False)\n",
    "grdp_clean['grdp'] = grdp_clean['grdp'].str.replace(',', '.', regex=False)\n",
    "grdp_clean['grdp'] = pd.to_numeric(grdp_clean['grdp'], errors='coerce')\n",
    "\n",
    "# Merge GRDP data by province + year\n",
    "merged_df = merged_df.merge(grdp_clean, on=['province', 'year'], how='left')\n",
    "\n",
    "print(f\"‚úÖ After merging GRDP: {merged_df.shape}\")\n",
    "print(f\"   GRDP coverage: {merged_df['grdp'].notna().sum() / len(merged_df) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f4aac10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ After merging area/population: (544956, 30)\n",
      "   Population coverage: 81.4%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.14: GH√âP AREA & POPULATION DATA (theo province + year)\n",
    "# =============================================================================\n",
    "\n",
    "# Clean area/population data\n",
    "area_pop_clean = area_pop_df[['NƒÉm', 'province_normalized', 'Di·ªán t√≠ch (Km2)', \n",
    "                              'D√¢n s·ªë trung b√¨nh (ngh√¨n)', 'M·∫≠t ƒë·ªô d√¢n s·ªë (ng∆∞·ªùi/km2)']].copy()\n",
    "area_pop_clean.columns = ['year', 'province', 'area_km2', 'population_thousand', 'density']\n",
    "\n",
    "# Merge area/population data by province + year\n",
    "merged_df = merged_df.merge(area_pop_clean, on=['province', 'year'], how='left')\n",
    "\n",
    "print(f\"‚úÖ After merging area/population: {merged_df.shape}\")\n",
    "print(f\"   Population coverage: {merged_df['population_thousand'].notna().sum() / len(merged_df) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baa7bd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing duplicates: (544956, 30)\n",
      "After removing duplicates: (174060, 30)\n",
      "‚úÖ After merging destination stats: (174060, 35)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.15: GH√âP DESTINATION STATISTICS & LO·∫†I B·ªé DUPLICATES\n",
    "# =============================================================================\n",
    "\n",
    "# Lo·∫°i b·ªè duplicates n·∫øu c√≥\n",
    "print(f\"Before removing duplicates: {merged_df.shape}\")\n",
    "merged_df = merged_df.drop_duplicates(subset=['date_parsed', 'destination'])\n",
    "print(f\"After removing duplicates: {merged_df.shape}\")\n",
    "\n",
    "# Clean destination statistics\n",
    "dest_stats_clean = dest_stats_df.rename(columns={\n",
    "    'Destination': 'destination',\n",
    "    'Mean': 'dest_mean_traffic',\n",
    "    'Median': 'dest_median_traffic', \n",
    "    'Max': 'dest_max_traffic',\n",
    "    'Std Dev': 'dest_std_traffic',\n",
    "    'Coverage %': 'dest_coverage_pct'\n",
    "})\n",
    "\n",
    "# Merge destination statistics\n",
    "merged_df = merged_df.merge(\n",
    "    dest_stats_clean[['destination', 'dest_mean_traffic', 'dest_median_traffic', \n",
    "                      'dest_max_traffic', 'dest_std_traffic', 'dest_coverage_pct']], \n",
    "    on='destination', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ After merging destination stats: {merged_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91e1ff61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìã T√ìM T·∫ÆT D·ªÆ LI·ªÜU ƒê√É GH√âP HO√ÄN CH·ªàNH\n",
      "======================================================================\n",
      "\n",
      "üìä Dataset shape: (174060, 35)\n",
      "üìç Unique destinations: 967\n",
      "üó∫Ô∏è Unique provinces: 34\n",
      "üìÖ Date range: 2011-01-01 00:00:00 to 2025-12-01 00:00:00\n",
      "\n",
      "üìã ALL COLUMNS (35):\n",
      "   1. date_parsed\n",
      "   2. destination\n",
      "   3. traffic\n",
      "   4. province\n",
      "   5. temp_avg\n",
      "   6. rainfall\n",
      "   7. amplitude\n",
      "   8. Peak_Months\n",
      "   9. Primary_Peak_Month\n",
      "   10. has_strong_seasonality\n",
      "   11. year\n",
      "   12. month\n",
      "   13. quarter\n",
      "   14. region\n",
      "   15. latitude\n",
      "   16. longitude\n",
      "   17. distance_to_hanoi_km\n",
      "   18. distance_to_hcm_km\n",
      "   19. accommodation_count\n",
      "   20. restaurant_count\n",
      "   21. entertainment_count\n",
      "   22. healthcare_count\n",
      "   23. shop_count\n",
      "   24. youtube_views\n",
      "   25. youtube_likes\n",
      "   26. youtube_comments\n",
      "   27. grdp\n",
      "   28. area_km2\n",
      "   29. population_thousand\n",
      "   30. density\n",
      "   31. dest_mean_traffic\n",
      "   32. dest_median_traffic\n",
      "   33. dest_max_traffic\n",
      "   34. dest_std_traffic\n",
      "   35. dest_coverage_pct\n",
      "\n",
      "üíæ Saved complete merged data to: ../data/normalized/merged_tourism_data_complete.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.16: T√ìM T·∫ÆT V√Ä L∆ØU D·ªÆ LI·ªÜU HO√ÄN CH·ªàNH\n",
    "# =============================================================================\n",
    "\n",
    "# Final summary\n",
    "print(\"=\" * 70)\n",
    "print(\"üìã T√ìM T·∫ÆT D·ªÆ LI·ªÜU ƒê√É GH√âP HO√ÄN CH·ªàNH\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìä Dataset shape: {merged_df.shape}\")\n",
    "print(f\"üìç Unique destinations: {merged_df['destination'].nunique()}\")\n",
    "print(f\"üó∫Ô∏è Unique provinces: {merged_df['province'].nunique()}\")\n",
    "print(f\"üìÖ Date range: {merged_df['date_parsed'].min()} to {merged_df['date_parsed'].max()}\")\n",
    "\n",
    "print(f\"\\nüìã ALL COLUMNS ({len(merged_df.columns)}):\")\n",
    "for i, col in enumerate(merged_df.columns):\n",
    "    print(f\"   {i+1}. {col}\")\n",
    "\n",
    "# Save complete merged dataset\n",
    "output_path_full = '../data/normalized/merged_tourism_data_complete.csv'\n",
    "merged_df.to_csv(output_path_full, index=False)\n",
    "print(f\"\\nüíæ Saved complete merged data to: {output_path_full}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dea24a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è MISSING VALUES SUMMARY:\n",
      "             Column  Missing Count  Missing %\n",
      "          amplitude         153000      87.90\n",
      " Primary_Peak_Month         153000      87.90\n",
      "        Peak_Months         153000      87.90\n",
      "population_thousand          92832      53.33\n",
      "               grdp          92832      53.33\n",
      "            density          92832      53.33\n",
      "           area_km2          92832      53.33\n",
      "           temp_avg           1573       0.90\n",
      "           rainfall           1573       0.90\n",
      "\n",
      "üìã SAMPLE DATA:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_parsed</th>\n",
       "      <th>destination</th>\n",
       "      <th>traffic</th>\n",
       "      <th>province</th>\n",
       "      <th>temp_avg</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>amplitude</th>\n",
       "      <th>Peak_Months</th>\n",
       "      <th>Primary_Peak_Month</th>\n",
       "      <th>has_strong_seasonality</th>\n",
       "      <th>...</th>\n",
       "      <th>youtube_comments</th>\n",
       "      <th>grdp</th>\n",
       "      <th>area_km2</th>\n",
       "      <th>population_thousand</th>\n",
       "      <th>density</th>\n",
       "      <th>dest_mean_traffic</th>\n",
       "      <th>dest_median_traffic</th>\n",
       "      <th>dest_max_traffic</th>\n",
       "      <th>dest_std_traffic</th>\n",
       "      <th>dest_coverage_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>2025-02-01</td>\n",
       "      <td>Th√°c Khe V·∫±n</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Qu·∫£ng Ninh</td>\n",
       "      <td>16.210714</td>\n",
       "      <td>39.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.168183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>1.995143</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82995</th>\n",
       "      <td>2011-10-01</td>\n",
       "      <td>L√†ng tr·∫ßu V·ªã Th·ªßy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TP. C·∫ßn Th∆°</td>\n",
       "      <td>26.500000</td>\n",
       "      <td>190.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>3859</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121552</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>Su·ªëi Ti√™n Th√†nh ph·ªë H·ªì Ch√≠ Minh</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TP. H·ªì Ch√≠ Minh</td>\n",
       "      <td>27.206452</td>\n",
       "      <td>65.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>4517</td>\n",
       "      <td>322934.04</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>1138.3</td>\n",
       "      <td>574.6</td>\n",
       "      <td>0.056321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.380952</td>\n",
       "      <td>0.315810</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date_parsed                      destination  traffic         province  \\\n",
       "2594    2025-02-01                     Th√°c Khe V·∫±n      0.0       Qu·∫£ng Ninh   \n",
       "82995   2011-10-01                L√†ng tr·∫ßu V·ªã Th·ªßy      0.0      TP. C·∫ßn Th∆°   \n",
       "121552  2018-12-01  Su·ªëi Ti√™n Th√†nh ph·ªë H·ªì Ch√≠ Minh      0.0  TP. H·ªì Ch√≠ Minh   \n",
       "\n",
       "         temp_avg  rainfall  amplitude Peak_Months  Primary_Peak_Month  \\\n",
       "2594    16.210714      39.4        NaN         NaN                 NaN   \n",
       "82995   26.500000     190.7        NaN         NaN                 NaN   \n",
       "121552  27.206452      65.5        NaN         NaN                 NaN   \n",
       "\n",
       "        has_strong_seasonality  ...  youtube_comments       grdp  area_km2  \\\n",
       "2594                     False  ...               316        NaN       NaN   \n",
       "82995                    False  ...              3859        NaN       NaN   \n",
       "121552                   False  ...              4517  322934.04    1981.0   \n",
       "\n",
       "       population_thousand  density  dest_mean_traffic  dest_median_traffic  \\\n",
       "2594                   NaN      NaN           0.168183                  0.0   \n",
       "82995                  NaN      NaN           0.000000                  0.0   \n",
       "121552              1138.3    574.6           0.056321                  0.0   \n",
       "\n",
       "        dest_max_traffic  dest_std_traffic  dest_coverage_pct  \n",
       "2594           26.666667          1.995143           1.666667  \n",
       "82995           0.000000          0.000000           0.000000  \n",
       "121552          2.380952          0.315810           3.333333  \n",
       "\n",
       "[3 rows x 35 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.17: KI·ªÇM TRA MISSING VALUES\n",
    "# =============================================================================\n",
    "\n",
    "# Check missing values\n",
    "missing_summary = merged_df.isnull().sum()\n",
    "missing_pct = (missing_summary / len(merged_df) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_summary.index,\n",
    "    'Missing Count': missing_summary.values,\n",
    "    'Missing %': missing_pct.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing %', ascending=False)\n",
    "\n",
    "print(\"‚ö†Ô∏è MISSING VALUES SUMMARY:\")\n",
    "print(missing_df.to_string(index=False))\n",
    "\n",
    "# Show sample of final dataset\n",
    "print(\"\\nüìã SAMPLE DATA:\")\n",
    "merged_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b387dab",
   "metadata": {},
   "source": [
    "---\n",
    "# ‚úÖ HO√ÄN T·∫§T GH√âP D·ªÆ LI·ªÜU\n",
    "\n",
    "B√¢y gi·ªù b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng `merged_df` ho·∫∑c load t·ª´ file `merged_tourism_data.csv` cho c√°c ph√¢n t√≠ch ti·∫øp theo.\n",
    "\n",
    "**C·∫•u tr√∫c d·ªØ li·ªáu ƒë√£ gh√©p:**\n",
    "| Column | Description |\n",
    "|--------|-------------|\n",
    "| date_parsed | Ng√†y (monthly) |\n",
    "| destination | T√™n ƒëi·ªÉm ƒë·∫øn |\n",
    "| traffic | L∆∞·ª£ng t√¨m ki·∫øm |\n",
    "| province | T·ªânh/TP |\n",
    "| temp_avg | Nhi·ªát ƒë·ªô trung b√¨nh |\n",
    "| rainfall | L∆∞·ª£ng m∆∞a |\n",
    "| Amplitude | Bi√™n ƒë·ªô dao ƒë·ªông (seasonality) |\n",
    "| Peak_Months | Th√°ng cao ƒëi·ªÉm |\n",
    "| year, month, quarter | Time features |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a5a352",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ D·ªÆ LI·ªÜU CH√çNH (Core Data)\n",
    "\n",
    "| File | M√¥ t·∫£ | S·ªë d√≤ng | Columns ch√≠nh |\n",
    "|------|-------|---------|---------------|\n",
    "| `vietnam_destinations_normalized.csv` | **Traffic t√¨m ki·∫øm theo th√°ng** cho 967 ƒë·ªãa ƒëi·ªÉm (2011-2025) | 180 rows | date, [967 destination columns] |\n",
    "| `destinations_statistics.csv` | Th·ªëng k√™ t·ªïng h·ª£p cho m·ªói ƒë·ªãa ƒëi·ªÉm | 968 rows | Destination, Mean, Median, Max, Min, Std Dev, Non-zero Count, Coverage % |\n",
    "| `keyword_mapping_normalized.csv` | Mapping ƒë·ªãa ƒëi·ªÉm ‚Üí t·ªânh/th√†nh | 967 rows | original_name, normalized_name, province, province_normalized |\n",
    "| `vietnam_seasonal_destinations_strong.csv` | **ƒê·ªãa ƒëi·ªÉm c√≥ t√≠nh m√πa v·ª• m·∫°nh** | 117 rows | Destination, Peak_Months, Primary_Peak_Month, Amplitude, CV |\n",
    "\n",
    "### 2Ô∏è‚É£ D·ªÆ LI·ªÜU B·ªî TR·ª¢ (Supporting Data)\n",
    "\n",
    "| File | M√¥ t·∫£ | D√πng cho |\n",
    "|------|-------|----------|\n",
    "| `vietnam_weather_monthly_2011_2025.csv` | Th·ªùi ti·∫øt theo th√°ng (nhi·ªát ƒë·ªô, l∆∞·ª£ng m∆∞a) | Feature th·ªùi ti·∫øt |\n",
    "| `vietnam_festivals.csv` | L·ªÖ h·ªôi theo th√°ng (2018-2025) | Feature l·ªÖ h·ªôi |\n",
    "| `vietnam_youtube_province_aggregates.csv` | Views/Likes YouTube theo t·ªânh | Feature social media |\n",
    "| `vietnam_grdp_by_province.csv` | GRDP theo t·ªânh | Feature kinh t·∫ø |\n",
    "| `vietnam_population.csv` | D√¢n s·ªë theo t·ªânh | Feature d√¢n s·ªë |\n",
    "| `vietnam_regions_with_distances.csv` | Kho·∫£ng c√°ch t·ª´ HN/HCM | Feature ƒë·ªãa l√Ω |\n",
    "| `vietnam_accommodation.csv` | S·ªë l∆∞·ª£ng kh√°ch s·∫°n/nh√† ngh·ªâ | Feature h·∫° t·∫ßng |\n",
    "| `vietnam_restaurants.csv` | S·ªë l∆∞·ª£ng nh√† h√†ng | Feature h·∫° t·∫ßng |\n",
    "| `vietnam_entertainment.csv` | ƒêi·ªÉm vui ch∆°i gi·∫£i tr√≠ | Feature h·∫° t·∫ßng |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2cfcfd",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¨ PH·∫¶N 1: KH√ÅM PH√Å D·ªÆ LI·ªÜU (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e769b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# C√†i ƒë·∫∑t hi·ªÉn th·ªã\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu\n",
    "DATA_PATH = '../data/normalized/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e134312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load d·ªØ li·ªáu ch√≠nh - Traffic t√¨m ki·∫øm\n",
    "df_traffic = pd.read_csv(DATA_PATH + 'vietnam_destinations_normalized.csv')\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi c·ªôt date\n",
    "# Format: 'thg X YYYY' -> datetime\n",
    "def parse_vietnamese_date(date_str):\n",
    "    \"\"\"Chuy·ªÉn ƒë·ªïi format 'thg 1 2011' th√†nh datetime\"\"\"\n",
    "    parts = date_str.split()\n",
    "    month = int(parts[1])\n",
    "    year = int(parts[2])\n",
    "    return pd.Timestamp(year=year, month=month, day=1)\n",
    "\n",
    "df_traffic['date'] = df_traffic['date'].apply(parse_vietnamese_date)\n",
    "df_traffic = df_traffic.set_index('date')\n",
    "\n",
    "print(f\"üìÖ Kho·∫£ng th·ªùi gian: {df_traffic.index.min()} ƒë·∫øn {df_traffic.index.max()}\")\n",
    "print(f\"üìç S·ªë ƒë·ªãa ƒëi·ªÉm: {len(df_traffic.columns)}\")\n",
    "print(f\"üìä S·ªë th√°ng d·ªØ li·ªáu: {len(df_traffic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ba3865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load th·ªëng k√™ ƒë·ªãa ƒëi·ªÉm\n",
    "df_stats = pd.read_csv(DATA_PATH + 'destinations_statistics.csv')\n",
    "print(\"\\nüìä Ph√¢n ph·ªëi Mean Traffic:\")\n",
    "print(df_stats['Mean'].describe())\n",
    "\n",
    "# Ph√¢n lo·∫°i ƒë·ªãa ƒëi·ªÉm theo m·ª©c traffic\n",
    "df_stats['traffic_level'] = pd.cut(\n",
    "    df_stats['Mean'], \n",
    "    bins=[0, 1, 10, 50, 100, float('inf')],\n",
    "    labels=['R·∫•t th·∫•p', 'Th·∫•p', 'Trung b√¨nh', 'Cao', 'R·∫•t cao']\n",
    ")\n",
    "print(\"\\nüìà Ph√¢n lo·∫°i ƒë·ªãa ƒëi·ªÉm theo traffic:\")\n",
    "print(df_stats['traffic_level'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54918d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load d·ªØ li·ªáu m√πa v·ª•\n",
    "df_seasonal = pd.read_csv(DATA_PATH + 'vietnam_seasonal_destinations_strong.csv')\n",
    "print(f\"\\nüóìÔ∏è S·ªë ƒë·ªãa ƒëi·ªÉm c√≥ t√≠nh m√πa v·ª• m·∫°nh: {len(df_seasonal)}\")\n",
    "\n",
    "# Top 10 ƒë·ªãa ƒëi·ªÉm c√≥ t√≠nh m√πa v·ª• m·∫°nh nh·∫•t\n",
    "print(\"\\nüî• Top 10 ƒë·ªãa ƒëi·ªÉm c√≥ bi√™n ƒë·ªô m√πa v·ª• cao nh·∫•t:\")\n",
    "df_seasonal.nlargest(10, 'Amplitude (Median Peak/Trough)')[['Destination', 'Peak_Months', 'Amplitude (Median Peak/Trough)', 'Primary_Peak_Month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaadf9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Mapping ƒë·ªãa ƒëi·ªÉm v·ªõi t·ªânh/th√†nh\n",
    "df_mapping = pd.read_csv(DATA_PATH + 'keyword_mapping_normalized.csv')\n",
    "print(\"\\nüìç Ph√¢n b·ªë ƒë·ªãa ƒëi·ªÉm theo t·ªânh/th√†nh (Top 15):\")\n",
    "print(df_mapping['province_normalized'].value_counts().head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb05fd8",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¨ PH·∫¶N 2: PH√ÇN T√çCH XU H∆Ø·ªöNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6846c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Ph√¢n t√≠ch xu h∆∞·ªõng t·ªïng th·ªÉ\n",
    "# T√≠nh t·ªïng traffic theo th√°ng\n",
    "total_traffic = df_traffic.sum(axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Xu h∆∞·ªõng t·ªïng\n",
    "axes[0].plot(total_traffic.index, total_traffic.values, linewidth=2, color='blue')\n",
    "axes[0].set_title('üìà Xu h∆∞·ªõng Traffic T√¨m Ki·∫øm Du L·ªãch T·ªïng Th·ªÉ (2011-2025)', fontsize=14)\n",
    "axes[0].set_xlabel('NƒÉm')\n",
    "axes[0].set_ylabel('T·ªïng Traffic')\n",
    "axes[0].axhline(y=total_traffic.mean(), color='red', linestyle='--', label=f'Trung b√¨nh: {total_traffic.mean():.0f}')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Ph√¢n t√≠ch theo th√°ng\n",
    "monthly_avg = df_traffic.groupby(df_traffic.index.month).mean().mean(axis=1)\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, 12))\n",
    "bars = axes[1].bar(range(1, 13), monthly_avg.values, color=colors)\n",
    "axes[1].set_title('üìä Traffic Trung B√¨nh Theo Th√°ng', fontsize=14)\n",
    "axes[1].set_xlabel('Th√°ng')\n",
    "axes[1].set_ylabel('Traffic Trung B√¨nh')\n",
    "axes[1].set_xticks(range(1, 13))\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105b913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Ph√¢n t√≠ch YoY Growth (Year-over-Year)\n",
    "# T√≠nh growth rate nƒÉm sau so v·ªõi nƒÉm tr∆∞·ªõc\n",
    "yearly_traffic = df_traffic.groupby(df_traffic.index.year).sum().sum(axis=1)\n",
    "yearly_growth = yearly_traffic.pct_change() * 100\n",
    "\n",
    "print(\"üìà TƒÉng tr∆∞·ªüng Traffic YoY (%):\")\n",
    "for year, growth in yearly_growth.dropna().items():\n",
    "    emoji = \"üü¢\" if growth > 0 else \"üî¥\"\n",
    "    print(f\"{emoji} {year}: {growth:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e20986",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¨ PH·∫¶N 3: FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a7464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 T·∫°o features t·ª´ d·ªØ li·ªáu th·ªùi ti·∫øt\n",
    "df_weather = pd.read_csv(DATA_PATH + 'vietnam_weather_monthly_2011_2025.csv')\n",
    "df_weather['date'] = pd.to_datetime(df_weather['date'])\n",
    "\n",
    "# T·∫°o features th·ªùi ti·∫øt theo t·ªânh v√† th√°ng\n",
    "weather_features = df_weather.groupby(['province', df_weather['date'].dt.to_period('M')]).agg({\n",
    "    'temp_avg': 'mean',\n",
    "    'rainfall': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "print(\"üìä Features Th·ªùi Ti·∫øt:\")\n",
    "print(weather_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d438b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 T·∫°o features t·ª´ l·ªÖ h·ªôi\n",
    "df_festivals = pd.read_csv(DATA_PATH + 'vietnam_festivals.csv')\n",
    "\n",
    "# ƒê·∫øm s·ªë l·ªÖ h·ªôi theo t·ªânh v√† th√°ng\n",
    "festival_columns = [col for col in df_festivals.columns if col.startswith('month_gregorian')]\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi th√†nh format d√†i\n",
    "festival_long = df_festivals.melt(\n",
    "    id_vars=['province', 'festival_name', 'type', 'province_normalized'],\n",
    "    value_vars=festival_columns,\n",
    "    var_name='year_col',\n",
    "    value_name='month'\n",
    ")\n",
    "\n",
    "# ƒê·∫øm s·ªë l·ªÖ h·ªôi\n",
    "festival_long['month'] = pd.to_datetime(festival_long['month'])\n",
    "festival_count = festival_long.groupby(['province_normalized', festival_long['month'].dt.to_period('M')]).size().reset_index(name='festival_count')\n",
    "\n",
    "print(\"üìä S·ªë l·ªÖ h·ªôi theo t·ªânh v√† th√°ng:\")\n",
    "print(festival_count.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c3999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 T·∫°o features lag v√† rolling\n",
    "def create_time_features(series, destination_name):\n",
    "    \"\"\"T·∫°o features th·ªùi gian cho m·ªôt ƒë·ªãa ƒëi·ªÉm\"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    df['traffic'] = series\n",
    "    \n",
    "    # Lag features (th√°ng tr∆∞·ªõc)\n",
    "    for i in [1, 2, 3, 6, 12]:\n",
    "        df[f'lag_{i}m'] = series.shift(i)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    df['rolling_mean_3m'] = series.rolling(window=3).mean()\n",
    "    df['rolling_mean_6m'] = series.rolling(window=6).mean()\n",
    "    df['rolling_std_3m'] = series.rolling(window=3).std()\n",
    "    \n",
    "    # Growth rate\n",
    "    df['mom_growth'] = series.pct_change()  # Month-over-Month\n",
    "    df['yoy_growth'] = series.pct_change(12)  # Year-over-Year\n",
    "    \n",
    "    # Seasonal features\n",
    "    df['month'] = series.index.month\n",
    "    df['quarter'] = series.index.quarter\n",
    "    df['year'] = series.index.year\n",
    "    \n",
    "    # Cyclical encoding cho month\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Demo v·ªõi m·ªôt ƒë·ªãa ƒëi·ªÉm\n",
    "sample_dest = 'H·ªì Ho√†n Ki·∫øm'\n",
    "if sample_dest in df_traffic.columns:\n",
    "    sample_features = create_time_features(df_traffic[sample_dest], sample_dest)\n",
    "    print(f\"üìä Features cho {sample_dest}:\")\n",
    "    print(sample_features.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d70a1e",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§ñ PH·∫¶N 4: X√ÇY D·ª∞NG M√î H√åNH D·ª∞ ƒêO√ÅN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1efe665",
   "metadata": {},
   "source": [
    "### üìã CHI·∫æN L∆Ø·ª¢C M√î H√åNH\n",
    "\n",
    "#### A. C√°c M√¥ H√¨nh ƒê·ªÅ Xu·∫•t:\n",
    "\n",
    "| M√¥ h√¨nh | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm | Ph√π h·ª£p khi |\n",
    "|---------|---------|------------|-------------|\n",
    "| **SARIMA** | B·∫Øt ƒë∆∞·ª£c t√≠nh m√πa v·ª•, interpretable | Ch·ªâ d√πng 1 chu·ªói, linear | D·ªØ li·ªáu c√≥ m√πa v·ª• r√µ r√†ng |\n",
    "| **Prophet** | D·ªÖ d√πng, t·ª± ƒë·ªông detect seasonality | C·∫ßn tune nhi·ªÅu | Time series v·ªõi trend v√† m√πa v·ª• |\n",
    "| **XGBoost** | X·ª≠ l√Ω nhi·ªÅu features, robust | C·∫ßn feature engineering | C√≥ nhi·ªÅu features ngo·∫°i sinh |\n",
    "| **LSTM** | B·∫Øt ƒë∆∞·ª£c pattern ph·ª©c t·∫°p | C·∫ßn nhi·ªÅu data, kh√≥ tune | D·ªØ li·ªáu l·ªõn, pattern ph·ª©c t·∫°p |\n",
    "| **Ensemble** | K·∫øt h·ª£p ∆∞u ƒëi·ªÉm c√°c m√¥ h√¨nh | Ph·ª©c t·∫°p | Production system |\n",
    "\n",
    "#### B. Chi·∫øn L∆∞·ª£c D·ª± ƒêo√°n Ng·∫Øn H·∫°n (1-3 th√°ng):\n",
    "\n",
    "1. **M√¥ h√¨nh c·∫•p ƒë·ªãa ƒëi·ªÉm**: D·ª± ƒëo√°n t·ª´ng destination ri√™ng bi·ªát\n",
    "2. **M√¥ h√¨nh c·∫•p t·ªânh**: Aggregate theo t·ªânh r·ªìi d·ª± ƒëo√°n\n",
    "3. **M√¥ h√¨nh k·∫øt h·ª£p**: D·ª± ƒëo√°n t·ªïng + ph√¢n b·ªï theo t·ªâ l·ªá l·ªãch s·ª≠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af4ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 C√†i ƒë·∫∑t th√™m th∆∞ vi·ªán (n·∫øu c·∫ßn)\n",
    "# !pip install prophet statsmodels xgboost scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 M√¥ h√¨nh SARIMA cho d·ª± ƒëo√°n ng·∫Øn h·∫°n\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def build_sarima_model(series, train_size=0.8):\n",
    "    \"\"\"\n",
    "    X√¢y d·ª±ng m√¥ h√¨nh SARIMA cho m·ªôt chu·ªói th·ªùi gian\n",
    "    \"\"\"\n",
    "    # Chia train/test\n",
    "    n = len(series)\n",
    "    train = series[:int(n * train_size)]\n",
    "    test = series[int(n * train_size):]\n",
    "    \n",
    "    # Fit model v·ªõi SARIMA(1,1,1)(1,1,1,12) - seasonal period = 12 th√°ng\n",
    "    try:\n",
    "        model = SARIMAX(\n",
    "            train, \n",
    "            order=(1, 1, 1),\n",
    "            seasonal_order=(1, 1, 1, 12),\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False\n",
    "        )\n",
    "        fitted = model.fit(disp=False)\n",
    "        \n",
    "        # D·ª± ƒëo√°n\n",
    "        predictions = fitted.forecast(len(test))\n",
    "        \n",
    "        # ƒê√°nh gi√°\n",
    "        mae = mean_absolute_error(test, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(test, predictions))\n",
    "        \n",
    "        return {\n",
    "            'model': fitted,\n",
    "            'predictions': predictions,\n",
    "            'actual': test,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Demo v·ªõi m·ªôt ƒë·ªãa ƒëi·ªÉm c√≥ traffic cao\n",
    "high_traffic_destinations = df_stats.nlargest(10, 'Mean')['Destination'].tolist()\n",
    "print(\"üèÜ Top 10 ƒë·ªãa ƒëi·ªÉm c√≥ traffic cao nh·∫•t:\")\n",
    "for i, dest in enumerate(high_traffic_destinations, 1):\n",
    "    mean_val = df_stats[df_stats['Destination'] == dest]['Mean'].values[0]\n",
    "    print(f\"  {i}. {dest}: {mean_val:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0997cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 XGBoost v·ªõi features ƒë·∫ßy ƒë·ªß\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def prepare_ml_dataset(df_traffic, df_mapping, destination):\n",
    "    \"\"\"\n",
    "    Chu·∫©n b·ªã dataset cho ML model\n",
    "    \"\"\"\n",
    "    if destination not in df_traffic.columns:\n",
    "        return None\n",
    "    \n",
    "    series = df_traffic[destination]\n",
    "    df = create_time_features(series, destination)\n",
    "    \n",
    "    # Th√™m th√¥ng tin t·ªânh/th√†nh\n",
    "    province_info = df_mapping[df_mapping['normalized_name'] == destination]\n",
    "    if len(province_info) > 0:\n",
    "        province = province_info['province_normalized'].values[0]\n",
    "        df['province'] = province\n",
    "    \n",
    "    # Drop NaN\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Functions ƒë√£ s·∫µn s√†ng cho training model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5731182",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä PH·∫¶N 5: PH√ÇN T√çCH CHUY√äN S√ÇU CHO D·ª∞ ƒêO√ÅN NG·∫ÆN H·∫†N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3016ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 X√°c ƒë·ªãnh ƒë·ªãa ƒëi·ªÉm c√≥ ti·ªÅm nƒÉng tƒÉng tr∆∞·ªüng trong k·ª≥ t·ªõi\n",
    "def identify_trending_destinations(df_traffic, df_seasonal, n_months=3):\n",
    "    \"\"\"\n",
    "    X√°c ƒë·ªãnh ƒë·ªãa ƒëi·ªÉm c√≥ kh·∫£ nƒÉng c√≥ traffic cao trong n th√°ng t·ªõi\n",
    "    d·ª±a tr√™n:\n",
    "    1. Peak months c·ªßa ƒë·ªãa ƒëi·ªÉm ƒë√≥\n",
    "    2. Trend g·∫ßn ƒë√¢y\n",
    "    3. YoY comparison\n",
    "    \"\"\"\n",
    "    current_month = datetime.now().month\n",
    "    upcoming_months = [(current_month + i) % 12 or 12 for i in range(1, n_months + 1)]\n",
    "    \n",
    "    print(f\"üóìÔ∏è Ph√¢n t√≠ch cho {n_months} th√°ng t·ªõi: {upcoming_months}\")\n",
    "    \n",
    "    potential_destinations = []\n",
    "    \n",
    "    for _, row in df_seasonal.iterrows():\n",
    "        dest = row['Destination']\n",
    "        peak_months = eval(row['Peak_Months_List']) if isinstance(row['Peak_Months_List'], str) else []\n",
    "        \n",
    "        # Check n·∫øu peak months n·∫±m trong upcoming months\n",
    "        overlap = set(peak_months) & set(upcoming_months)\n",
    "        if overlap:\n",
    "            potential_destinations.append({\n",
    "                'destination': dest,\n",
    "                'peak_months': peak_months,\n",
    "                'upcoming_peak_months': list(overlap),\n",
    "                'amplitude': row['Amplitude (Median Peak/Trough)'],\n",
    "                'primary_peak': row['Primary_Peak_Month']\n",
    "            })\n",
    "    \n",
    "    result_df = pd.DataFrame(potential_destinations)\n",
    "    return result_df.sort_values('amplitude', ascending=False)\n",
    "\n",
    "# Ch·∫°y ph√¢n t√≠ch\n",
    "trending = identify_trending_destinations(df_traffic, df_seasonal, n_months=3)\n",
    "print(f\"\\nüî• S·ªë ƒë·ªãa ƒëi·ªÉm c√≥ peak trong 3 th√°ng t·ªõi: {len(trending)}\")\n",
    "print(\"\\nüìà Top 20 ƒë·ªãa ƒëi·ªÉm ti·ªÅm nƒÉng:\")\n",
    "trending.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d20bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Ph√¢n t√≠ch correlation v·ªõi c√°c y·∫øu t·ªë\n",
    "def analyze_traffic_drivers(df_traffic, df_weather, df_mapping, destination):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch c√°c y·∫øu t·ªë ·∫£nh h∆∞·ªüng ƒë·∫øn traffic c·ªßa m·ªôt ƒë·ªãa ƒëi·ªÉm\n",
    "    \"\"\"\n",
    "    if destination not in df_traffic.columns:\n",
    "        return None\n",
    "    \n",
    "    # L·∫•y province\n",
    "    province_info = df_mapping[df_mapping['normalized_name'] == destination]\n",
    "    if len(province_info) == 0:\n",
    "        return None\n",
    "    \n",
    "    province = province_info['province_normalized'].values[0]\n",
    "    \n",
    "    # L·∫•y traffic\n",
    "    traffic = df_traffic[destination].reset_index()\n",
    "    traffic.columns = ['date', 'traffic']\n",
    "    \n",
    "    # Merge v·ªõi weather\n",
    "    weather_subset = df_weather[df_weather['province'] == province].copy()\n",
    "    if len(weather_subset) == 0:\n",
    "        return None\n",
    "    \n",
    "    # T√≠nh correlation\n",
    "    traffic['month'] = traffic['date'].dt.month\n",
    "    weather_subset['month'] = pd.to_datetime(weather_subset['date']).dt.month\n",
    "    \n",
    "    # Th·ªëng k√™ ƒë∆°n gi·∫£n\n",
    "    result = {\n",
    "        'destination': destination,\n",
    "        'province': province,\n",
    "        'avg_traffic': traffic['traffic'].mean(),\n",
    "        'peak_month': traffic.groupby('month')['traffic'].mean().idxmax(),\n",
    "        'low_month': traffic.groupby('month')['traffic'].mean().idxmin()\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ Function analyze_traffic_drivers ƒë√£ s·∫µn s√†ng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3efeb5",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ PH·∫¶N 6: ƒê·ªÄ XU·∫§T & K·∫æT LU·∫¨N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d9be1b",
   "metadata": {},
   "source": [
    "### üìã H∆Ø·ªöNG PH√ÇN T√çCH CHI TI·∫æT\n",
    "\n",
    "#### B∆Ø·ªöC 1: Data Understanding & Cleaning\n",
    "1. ‚úÖ Load v√† kh√°m ph√° t·∫•t c·∫£ datasets\n",
    "2. ‚úÖ X·ª≠ l√Ω missing values\n",
    "3. ‚úÖ Chu·∫©n h√≥a t√™n t·ªânh/th√†nh\n",
    "4. ‚úÖ Merge c√°c ngu·ªìn d·ªØ li·ªáu\n",
    "\n",
    "#### B∆Ø·ªöC 2: EDA Chuy√™n S√¢u\n",
    "1. Ph√¢n t√≠ch xu h∆∞·ªõng t·ªïng th·ªÉ traffic\n",
    "2. Ph√¢n t√≠ch m√πa v·ª• theo th√°ng/qu√Ω\n",
    "3. Ph√¢n t√≠ch theo v√πng mi·ªÅn\n",
    "4. X√°c ƒë·ªãnh ƒë·ªãa ƒëi·ªÉm c√≥ t√≠nh m√πa v·ª• m·∫°nh\n",
    "5. Ph√¢n t√≠ch correlation v·ªõi th·ªùi ti·∫øt, l·ªÖ h·ªôi\n",
    "\n",
    "#### B∆Ø·ªöC 3: Feature Engineering\n",
    "1. **Time features**: Lag, rolling mean/std, growth rates\n",
    "2. **Seasonal features**: Month sin/cos, quarter, is_holiday\n",
    "3. **External features**: Weather, festivals, GRDP\n",
    "4. **Infrastructure features**: S·ªë kh√°ch s·∫°n, nh√† h√†ng\n",
    "\n",
    "#### B∆Ø·ªöC 4: Modeling\n",
    "1. **Baseline**: SARIMA cho t·ª´ng ƒë·ªãa ƒëi·ªÉm\n",
    "2. **Advanced**: XGBoost/LightGBM v·ªõi all features\n",
    "3. **Ensemble**: K·∫øt h·ª£p nhi·ªÅu m√¥ h√¨nh\n",
    "4. **Evaluation**: MAE, RMSE, MAPE\n",
    "\n",
    "#### B∆Ø·ªöC 5: Prediction & Deployment\n",
    "1. D·ª± ƒëo√°n traffic 1-3 th√°ng t·ªõi\n",
    "2. Ranking ƒë·ªãa ƒëi·ªÉm ti·ªÅm nƒÉng\n",
    "3. ƒê·ªÅ xu·∫•t chi·∫øn l∆∞·ª£c marketing\n",
    "4. Dashboard monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974a1bdf",
   "metadata": {},
   "source": [
    "### üìä METRICS ƒê√ÅNH GI√Å\n",
    "\n",
    "| Metric | C√¥ng th·ª©c | √ù nghƒ©a |\n",
    "|--------|-----------|----------|\n",
    "| **MAE** | Mean Absolute Error | Sai s·ªë trung b√¨nh tuy·ªát ƒë·ªëi |\n",
    "| **RMSE** | Root Mean Squared Error | Penalize l·ªói l·ªõn nhi·ªÅu h∆°n |\n",
    "| **MAPE** | Mean Absolute Percentage Error | Sai s·ªë % - d·ªÖ hi·ªÉu |\n",
    "| **Hit Rate** | % ƒë·ªãa ƒëi·ªÉm predicted top-K n·∫±m trong actual top-K | ƒê√°nh gi√° ranking |\n",
    "\n",
    "### üéØ ·ª®NG D·ª§NG TH·ª∞C T·∫æ\n",
    "\n",
    "1. **Marketing Campaign**: Target ƒë·ªãa ƒëi·ªÉm c√≥ traffic predicted cao\n",
    "2. **Resource Planning**: Chu·∫©n b·ªã h·∫° t·∫ßng, nh√¢n l·ª±c\n",
    "3. **Content Creation**: T·∫°o content cho ƒë·ªãa ƒëi·ªÉm s·∫Øp peak\n",
    "4. **Promotion Timing**: Ch·∫°y promotion tr∆∞·ªõc khi peak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3449fd24",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ NEXT STEPS\n",
    "\n",
    "1. **Ho√†n thi·ªán EDA**: Ph√¢n t√≠ch s√¢u h∆°n t·ª´ng lo·∫°i ƒë·ªãa ƒëi·ªÉm\n",
    "2. **Build models**: Implement SARIMA, Prophet, XGBoost\n",
    "3. **Validate**: Cross-validation, backtesting\n",
    "4. **Production**: API, Dashboard\n",
    "5. **Monitor**: Theo d√µi v√† c·∫≠p nh·∫≠t model\n",
    "\n",
    "---\n",
    "\n",
    "**üìù Note**: Notebook n√†y l√† template h∆∞·ªõng d·∫´n. C·∫ßn ch·∫°y v√† tinh ch·ªânh theo d·ªØ li·ªáu th·ª±c t·∫ø."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b3f095",
   "metadata": {},
   "source": [
    "---\n",
    "# üìä PH·∫¶N 7: S·ª¨ D·ª§NG DATASET ƒê√É MERGE S·∫¥N\n",
    "\n",
    "## ∆Øu ƒëi·ªÉm c·ªßa `merged_tourism_data_complete.csv`\n",
    "- **174,060 d√≤ng** (m·ªói destination √ó m·ªói th√°ng)\n",
    "- **35 features** ƒë√£ ƒë∆∞·ª£c t√≠nh to√°n s·∫µn\n",
    "- ƒê√£ x·ª≠ l√Ω missing values v√† chu·∫©n h√≥a t√™n t·ªânh\n",
    "\n",
    "### C√°c c·ªôt quan tr·ªçng:\n",
    "| Nh√≥m | C·ªôt | M√¥ t·∫£ |\n",
    "|------|-----|-------|\n",
    "| **Target** | `traffic` | S·ªë l∆∞·ª£ng t√¨m ki·∫øm (ƒë·ªÉ d·ª± ƒëo√°n) |\n",
    "| **Time** | `date_parsed`, `year`, `month`, `quarter` | Th·ªùi gian |\n",
    "| **Location** | `destination`, `province`, `region`, `latitude`, `longitude` | V·ªã tr√≠ |\n",
    "| **Weather** | `temp_avg`, `rainfall` | Th·ªùi ti·∫øt th√°ng ƒë√≥ |\n",
    "| **Seasonality** | `amplitude`, `Peak_Months`, `has_strong_seasonality` | ƒê·∫∑c ƒëi·ªÉm m√πa v·ª• |\n",
    "| **Infrastructure** | `accommodation_count`, `restaurant_count`, `entertainment_count`, `healthcare_count`, `shop_count` | H·∫° t·∫ßng du l·ªãch |\n",
    "| **Social** | `youtube_views`, `youtube_likes`, `youtube_comments` | T√≠n hi·ªáu social media |\n",
    "| **Economy** | `grdp`, `area_km2`, `population_thousand`, `density` | Kinh t·∫ø t·ªânh |\n",
    "| **Distance** | `distance_to_hanoi_km`, `distance_to_hcm_km` | Kho·∫£ng c√°ch ƒë·∫øn 2 TP l·ªõn |\n",
    "| **Stats** | `dest_mean_traffic`, `dest_median_traffic`, `dest_max_traffic`, `dest_std_traffic`, `dest_coverage_pct` | Th·ªëng k√™ c·ªßa destination |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc3b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# B∆Ø·ªöC 7.1: LOAD DATASET ƒê√É MERGE S·∫¥N\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load merged dataset\n",
    "df = pd.read_csv('../data/normalized/merged_tourism_data_complete.csv')\n",
    "df['date_parsed'] = pd.to_datetime(df['date_parsed'])\n",
    "\n",
    "print(\"üìä DATASET OVERVIEW\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Date range: {df['date_parsed'].min()} ‚Üí {df['date_parsed'].max()}\")\n",
    "print(f\"   Unique destinations: {df['destination'].nunique()}\")\n",
    "print(f\"   Unique provinces: {df['province'].nunique()}\")\n",
    "print(f\"\\nüìã Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nüìà Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc0ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# B∆Ø·ªöC 7.2: PH√ÇN T√çCH S∆† B·ªò TARGET VARIABLE (TRAFFIC)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üéØ TARGET VARIABLE: traffic\\n\")\n",
    "print(df['traffic'].describe())\n",
    "\n",
    "# Ph√¢n t√≠ch ph√¢n ph·ªëi\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. Distribution (log scale v√¨ highly skewed)\n",
    "ax1 = axes[0]\n",
    "df_nonzero = df[df['traffic'] > 0]['traffic']\n",
    "ax1.hist(np.log1p(df_nonzero), bins=50, edgecolor='white')\n",
    "ax1.set_xlabel('log1p(traffic)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Traffic (log scale)')\n",
    "\n",
    "# 2. Zero-inflation analysis\n",
    "ax2 = axes[1]\n",
    "zero_pct = (df['traffic'] == 0).mean() * 100\n",
    "nonzero_pct = 100 - zero_pct\n",
    "ax2.bar(['Zero', 'Non-Zero'], [zero_pct, nonzero_pct], color=['#e74c3c', '#27ae60'])\n",
    "ax2.set_ylabel('Percentage')\n",
    "ax2.set_title(f'Zero-Inflation: {zero_pct:.1f}% l√† zero')\n",
    "\n",
    "# 3. Traffic theo th√°ng\n",
    "ax3 = axes[2]\n",
    "monthly_traffic = df.groupby('month')['traffic'].mean()\n",
    "ax3.bar(monthly_traffic.index, monthly_traffic.values, color='#3498db')\n",
    "ax3.set_xlabel('Month')\n",
    "ax3.set_ylabel('Average Traffic')\n",
    "ax3.set_title('Seasonality: Traffic theo th√°ng')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è L∆ØU √ù: {zero_pct:.1f}% d·ªØ li·ªáu = 0 ‚Üí C·∫ßn x·ª≠ l√Ω zero-inflation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971c3bc",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è B∆Ø·ªöC 7.3: FEATURE ENGINEERING CHO D·ª∞ ƒêO√ÅN NG·∫ÆN H·∫†N\n",
    "\n",
    "### Chi·∫øn l∆∞·ª£c Feature Engineering:\n",
    "\n",
    "| Lo·∫°i Feature | T√™n | C√¥ng th·ª©c | M·ª•c ƒë√≠ch |\n",
    "|--------------|-----|-----------|----------|\n",
    "| **Lag Features** | `lag_1m`, `lag_2m`, `lag_3m` | traffic(t-1), traffic(t-2), traffic(t-3) | Xu h∆∞·ªõng g·∫ßn ƒë√¢y |\n",
    "| **Rolling Features** | `rolling_3m_mean`, `rolling_3m_std` | mean/std c·ªßa 3 th√°ng g·∫ßn nh·∫•t | Trung b√¨nh ƒë·ªông |\n",
    "| **YoY Features** | `traffic_same_month_last_year` | traffic c·ªßa c√πng th√°ng nƒÉm tr∆∞·ªõc | T√≠nh m√πa v·ª• nƒÉm |\n",
    "| **Growth Rate** | `growth_1m`, `growth_yoy` | (t - t-1) / t-1 | T·ªëc ƒë·ªô tƒÉng tr∆∞·ªüng |\n",
    "| **Seasonal** | `month_sin`, `month_cos` | sin/cos c·ªßa th√°ng | Seasonal encoding |\n",
    "| **Is Peak** | `is_peak_month` | Th√°ng hi·ªán t·∫°i c√≥ trong Peak_Months kh√¥ng | Binary flag |\n",
    "| **Weather Delta** | `temp_anomaly` | temp_avg - temp trung b√¨nh th√°ng ƒë√≥ | Th·ªùi ti·∫øt b·∫•t th∆∞·ªùng |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bf5a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# B∆Ø·ªöC 7.3: T·∫†O FEATURES CHO MODELING\n",
    "# =============================================================================\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    T·∫°o features cho d·ª± ƒëo√°n traffic ng·∫Øn h·∫°n\n",
    "    \"\"\"\n",
    "    df = df.sort_values(['destination', 'date_parsed']).copy()\n",
    "    \n",
    "    # 1. LAG FEATURES - Quan tr·ªçng nh·∫•t cho d·ª± ƒëo√°n ng·∫Øn h·∫°n\n",
    "    for lag in [1, 2, 3, 6, 12]:\n",
    "        df[f'lag_{lag}m'] = df.groupby('destination')['traffic'].shift(lag)\n",
    "    \n",
    "    # 2. ROLLING FEATURES - Trung b√¨nh ƒë·ªông\n",
    "    df['rolling_3m_mean'] = df.groupby('destination')['traffic'].transform(\n",
    "        lambda x: x.shift(1).rolling(3, min_periods=1).mean()\n",
    "    )\n",
    "    df['rolling_3m_std'] = df.groupby('destination')['traffic'].transform(\n",
    "        lambda x: x.shift(1).rolling(3, min_periods=1).std()\n",
    "    )\n",
    "    df['rolling_6m_mean'] = df.groupby('destination')['traffic'].transform(\n",
    "        lambda x: x.shift(1).rolling(6, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    # 3. GROWTH RATE\n",
    "    df['growth_1m'] = (df['traffic'] - df['lag_1m']) / (df['lag_1m'] + 1)  # +1 ƒë·ªÉ tr√°nh chia 0\n",
    "    df['growth_yoy'] = (df['traffic'] - df['lag_12m']) / (df['lag_12m'] + 1)\n",
    "    \n",
    "    # 4. SEASONAL ENCODING (cyclical)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    \n",
    "    # 5. IS PEAK MONTH (d·ª±a tr√™n Peak_Months column)\n",
    "    def check_peak(row):\n",
    "        if pd.isna(row['Peak_Months']):\n",
    "            return 0\n",
    "        try:\n",
    "            peak_months = [int(x.strip()) for x in str(row['Peak_Months']).split(',')]\n",
    "            return 1 if row['month'] in peak_months else 0\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    df['is_peak_month'] = df.apply(check_peak, axis=1)\n",
    "    \n",
    "    # 6. WEATHER ANOMALY (so v·ªõi trung b√¨nh th√°ng ƒë√≥ c·ªßa t·ªânh)\n",
    "    monthly_avg_temp = df.groupby(['province', 'month'])['temp_avg'].transform('mean')\n",
    "    df['temp_anomaly'] = df['temp_avg'] - monthly_avg_temp\n",
    "    \n",
    "    # 7. RELATIVE TRAFFIC (so v·ªõi destination mean)\n",
    "    df['relative_traffic'] = df['traffic'] / (df['dest_mean_traffic'] + 1)\n",
    "    \n",
    "    # 8. YEAR TREND\n",
    "    df['year_trend'] = df['year'] - df['year'].min()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"üîß Creating features...\")\n",
    "df_features = create_features(df)\n",
    "print(f\"‚úÖ Shape after feature engineering: {df_features.shape}\")\n",
    "print(f\"‚úÖ New columns: {[c for c in df_features.columns if c not in df.columns]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d77b81",
   "metadata": {},
   "source": [
    "## üéØ B∆Ø·ªöC 7.4: TRAIN/TEST SPLIT CHO TIME SERIES\n",
    "\n",
    "### ‚ö†Ô∏è L∆ØU √ù QUAN TR·ªåNG:\n",
    "- **KH√îNG ƒë∆∞·ª£c d√πng random split** cho time series (data leakage)\n",
    "- Ph·∫£i chia theo th·ªùi gian: train tr√™n qu√° kh·ª©, test tr√™n t∆∞∆°ng lai\n",
    "- S·ª≠ d·ª•ng **Rolling Origin Cross-Validation** ƒë·ªÉ ƒë√°nh gi√°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8223f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# B∆Ø·ªöC 7.4: CHIA D·ªÆ LI·ªÜU TRAIN/VALIDATION/TEST\n",
    "# =============================================================================\n",
    "\n",
    "# Time-based split\n",
    "# Train: 2011-2022, Validation: 2023, Test: 2024-2025\n",
    "train_df = df_features[df_features['year'] <= 2022].copy()\n",
    "val_df = df_features[df_features['year'] == 2023].copy()\n",
    "test_df = df_features[df_features['year'] >= 2024].copy()\n",
    "\n",
    "print(\"üìä DATA SPLIT (Time-based):\")\n",
    "print(f\"   Train:      {train_df.shape[0]:,} rows ({train_df['year'].min()}-{train_df['year'].max()})\")\n",
    "print(f\"   Validation: {val_df.shape[0]:,} rows ({val_df['year'].min()}-{val_df['year'].max()})\")\n",
    "print(f\"   Test:       {test_df.shape[0]:,} rows ({test_df['year'].min()}-{test_df['year'].max()})\")\n",
    "\n",
    "# Lo·∫°i b·ªè c√°c d√≤ng c√≥ missing values do lag features (c√°c th√°ng ƒë·∫ßu ti√™n)\n",
    "print(f\"\\n‚ö†Ô∏è Missing values due to lag features:\")\n",
    "print(f\"   Train before dropna: {len(train_df)}\")\n",
    "train_df = train_df.dropna(subset=['lag_1m', 'lag_12m'])\n",
    "print(f\"   Train after dropna: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d268f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# B∆Ø·ªöC 7.5: ƒê·ªäNH NGHƒ®A FEATURES CHO MODEL\n",
    "# =============================================================================\n",
    "\n",
    "# Features to use\n",
    "FEATURE_COLS = [\n",
    "    # Lag features (quan tr·ªçng nh·∫•t)\n",
    "    'lag_1m', 'lag_2m', 'lag_3m', 'lag_6m', 'lag_12m',\n",
    "    \n",
    "    # Rolling statistics\n",
    "    'rolling_3m_mean', 'rolling_3m_std', 'rolling_6m_mean',\n",
    "    \n",
    "    # Time features\n",
    "    'month_sin', 'month_cos', 'quarter', 'year_trend',\n",
    "    \n",
    "    # Seasonality\n",
    "    'is_peak_month', 'has_strong_seasonality', 'amplitude',\n",
    "    \n",
    "    # Weather\n",
    "    'temp_avg', 'rainfall', 'temp_anomaly',\n",
    "    \n",
    "    # Infrastructure\n",
    "    'accommodation_count', 'restaurant_count', 'entertainment_count', \n",
    "    'shop_count', 'healthcare_count',\n",
    "    \n",
    "    # Location\n",
    "    'latitude', 'longitude', 'distance_to_hanoi_km', 'distance_to_hcm_km',\n",
    "    \n",
    "    # Social\n",
    "    'youtube_views', 'youtube_likes',\n",
    "    \n",
    "    # Economics\n",
    "    'grdp', 'population_thousand', 'density',\n",
    "    \n",
    "    # Destination stats\n",
    "    'dest_mean_traffic', 'dest_max_traffic', 'dest_coverage_pct'\n",
    "]\n",
    "\n",
    "TARGET = 'traffic'\n",
    "\n",
    "# Filter only columns that exist\n",
    "FEATURE_COLS = [c for c in FEATURE_COLS if c in df_features.columns]\n",
    "\n",
    "print(f\"üìã Features selected: {len(FEATURE_COLS)}\")\n",
    "print(f\"   {FEATURE_COLS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f507d3",
   "metadata": {},
   "source": [
    "## ü§ñ B∆Ø·ªöC 8: X√ÇY D·ª∞NG M√î H√åNH D·ª∞ ƒêO√ÅN\n",
    "\n",
    "### C√°c m√¥ h√¨nh ƒë·ªÅ xu·∫•t:\n",
    "\n",
    "| Model | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm | Khi n√†o d√πng |\n",
    "|-------|---------|------------|--------------|\n",
    "| **LightGBM/XGBoost** | Nhanh, x·ª≠ l√Ω t·ªët tabular data, feature importance | Kh√¥ng capture time dependency t·ªët | **Khuy·∫øn ngh·ªã ch√≠nh** |\n",
    "| **SARIMA/Prophet** | Capture seasonality t·ªët | Ch·ªâ fit 1 series, ch·∫≠m v·ªõi nhi·ªÅu destinations | Ph√¢n t√≠ch t·ª´ng ƒë·ªãa ƒëi·ªÉm |\n",
    "| **Random Forest** | Robust, √≠t tuning | Ch·∫≠m h∆°n gradient boosting | Baseline |\n",
    "| **Neural Network** | Flexible | C·∫ßn nhi·ªÅu data, kh√≥ interpret | Khi c√≥ nhi·ªÅu d·ªØ li·ªáu |\n",
    "\n",
    "### üéØ Chi·∫øn l∆∞·ª£c ƒë·ªÅ xu·∫•t:\n",
    "1. **Baseline**: Naive forecast (d√πng c√πng th√°ng nƒÉm tr∆∞·ªõc)\n",
    "2. **Main Model**: LightGBM v·ªõi all features\n",
    "3. **Ensemble**: K·∫øt h·ª£p LightGBM + Prophet cho top destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6401cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# B∆Ø·ªöC 8.1: BASELINE MODEL (Seasonal Naive)\n",
    "# =============================================================================\n",
    "\n",
    "def seasonal_naive_forecast(df, forecast_horizon=1):\n",
    "    \"\"\"\n",
    "    Baseline: D√πng traffic c·ªßa c√πng th√°ng nƒÉm tr∆∞·ªõc\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['prediction_naive'] = df['lag_12m']  # C√πng th√°ng nƒÉm tr∆∞·ªõc\n",
    "    return df\n",
    "\n",
    "# Apply baseline\n",
    "val_df_baseline = seasonal_naive_forecast(val_df)\n",
    "val_df_baseline = val_df_baseline.dropna(subset=['prediction_naive', 'traffic'])\n",
    "\n",
    "# ƒê√°nh gi√° baseline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "mae_baseline = mean_absolute_error(val_df_baseline['traffic'], val_df_baseline['prediction_naive'])\n",
    "rmse_baseline = np.sqrt(mean_squared_error(val_df_baseline['traffic'], val_df_baseline['prediction_naive']))\n",
    "mape_baseline = np.mean(np.abs((val_df_baseline['traffic'] - val_df_baseline['prediction_naive']) / \n",
    "                               (val_df_baseline['traffic'] + 1))) * 100\n",
    "\n",
    "print(\"üìä BASELINE (Seasonal Naive) Performance on Validation Set:\")\n",
    "print(f\"   MAE:  {mae_baseline:.2f}\")\n",
    "print(f\"   RMSE: {rmse_baseline:.2f}\")\n",
    "print(f\"   MAPE: {mape_baseline:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901f2aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# B∆Ø·ªöC 8.2: LIGHTGBM MODEL\n",
    "# =============================================================================\n",
    "\n",
    "# Install if needed\n",
    "# !pip install lightgbm\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Prepare data\n",
    "X_train = train_df[FEATURE_COLS].fillna(0)\n",
    "y_train = train_df[TARGET]\n",
    "X_val = val_df[FEATURE_COLS].fillna(0)\n",
    "y_val = val_df[TARGET]\n",
    "\n",
    "# LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'n_estimators': 500,\n",
    "    'early_stopping_rounds': 50\n",
    "}\n",
    "\n",
    "print(\"üöÄ Training LightGBM model...\")\n",
    "model_lgb = lgb.LGBMRegressor(**params)\n",
    "model_lgb.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='mae'\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred_lgb = model_lgb.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "mae_lgb = mean_absolute_error(y_val, y_pred_lgb)\n",
    "rmse_lgb = np.sqrt(mean_squared_error(y_val, y_pred_lgb))\n",
    "mape_lgb = np.mean(np.abs((y_val - y_pred_lgb) / (y_val + 1))) * 100\n",
    "\n",
    "print(f\"\\nüìä LIGHTGBM Performance on Validation Set:\")\n",
    "print(f\"   MAE:  {mae_lgb:.2f} (Baseline: {mae_baseline:.2f}, Improvement: {(1 - mae_lgb/mae_baseline)*100:.1f}%)\")\n",
    "print(f\"   RMSE: {rmse_lgb:.2f} (Baseline: {rmse_baseline:.2f})\")\n",
    "print(f\"   MAPE: {mape_lgb:.2f}% (Baseline: {mape_baseline:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb4713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# B∆Ø·ªöC 8.3: FEATURE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Get feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': FEATURE_COLS,\n",
    "    'importance': model_lgb.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_n = 20\n",
    "sns.barplot(data=importance_df.head(top_n), x='importance', y='feature', palette='viridis', ax=ax)\n",
    "ax.set_title(f'Top {top_n} Feature Importance')\n",
    "ax.set_xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Top 10 Most Important Features:\")\n",
    "print(importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b0051",
   "metadata": {},
   "source": [
    "## üéØ B∆Ø·ªöC 9: X√ÅC ƒê·ªäNH ƒêI·ªÇM ƒê·∫æN C√ì TRAFFIC CAO TRONG T∆Ø∆†NG LAI\n",
    "\n",
    "### M·ª•c ti√™u ch√≠nh: D·ª± ƒëo√°n Top-K ƒë·ªãa ƒëi·ªÉm c√≥ traffic cao nh·∫•t trong 1-3 th√°ng t·ªõi\n",
    "\n",
    "**Hai c√°ch ti·∫øp c·∫≠n:**\n",
    "1. **Regression**: D·ª± ƒëo√°n gi√° tr·ªã traffic ‚Üí Ranking theo gi√° tr·ªã d·ª± ƒëo√°n\n",
    "2. **Classification**: Ph√¢n lo·∫°i Top-K vs Rest ‚Üí Binary classification\n",
    "\n",
    "**Metrics ƒë√°nh gi√°:**\n",
    "- **Precision@K**: Bao nhi√™u % trong Top-K d·ª± ƒëo√°n th·ª±c s·ª± n·∫±m trong Top-K th·ª±c t·∫ø\n",
    "- **Recall@K**: Bao nhi√™u % c·ªßa Top-K th·ª±c t·∫ø ƒë∆∞·ª£c capture b·ªüi Top-K d·ª± ƒëo√°n\n",
    "- **NDCG**: ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e52e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# B∆Ø·ªöC 9.1: D·ª∞ ƒêO√ÅN V√Ä RANKING TOP DESTINATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def predict_and_rank(model, df, feature_cols, top_k=50):\n",
    "    \"\"\"\n",
    "    D·ª± ƒëo√°n traffic v√† ranking top-K destinations cho m·ªói th√°ng\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    X = df[feature_cols].fillna(0)\n",
    "    df['predicted_traffic'] = model.predict(X)\n",
    "    \n",
    "    # Rank within each month\n",
    "    df['rank_predicted'] = df.groupby('date_parsed')['predicted_traffic'].rank(ascending=False)\n",
    "    df['rank_actual'] = df.groupby('date_parsed')['traffic'].rank(ascending=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to validation set\n",
    "val_ranked = predict_and_rank(model_lgb, val_df, FEATURE_COLS, top_k=50)\n",
    "\n",
    "# T√≠nh Precision@K v√† Recall@K\n",
    "def calculate_ranking_metrics(df, k=50):\n",
    "    \"\"\"\n",
    "    T√≠nh precision@K, recall@K cho m·ªói th√°ng\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for date in df['date_parsed'].unique():\n",
    "        month_df = df[df['date_parsed'] == date]\n",
    "        \n",
    "        top_k_predicted = set(month_df[month_df['rank_predicted'] <= k]['destination'])\n",
    "        top_k_actual = set(month_df[month_df['rank_actual'] <= k]['destination'])\n",
    "        \n",
    "        if len(top_k_predicted) == 0 or len(top_k_actual) == 0:\n",
    "            continue\n",
    "            \n",
    "        overlap = len(top_k_predicted & top_k_actual)\n",
    "        precision = overlap / len(top_k_predicted)\n",
    "        recall = overlap / len(top_k_actual)\n",
    "        \n",
    "        results.append({\n",
    "            'date': date,\n",
    "            'precision@k': precision,\n",
    "            'recall@k': recall\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate ranking\n",
    "for k in [20, 50, 100]:\n",
    "    ranking_metrics = calculate_ranking_metrics(val_ranked, k=k)\n",
    "    avg_precision = ranking_metrics['precision@k'].mean()\n",
    "    avg_recall = ranking_metrics['recall@k'].mean()\n",
    "    print(f\"üìä K={k}: Precision@K={avg_precision:.2%}, Recall@K={avg_recall:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccbc06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# B∆Ø·ªöC 9.2: D·ª∞ B√ÅO CHO C√ÅC TH√ÅNG S·∫ÆP T·ªöI (PRODUCTION)\n",
    "# =============================================================================\n",
    "\n",
    "def forecast_upcoming_months(model, df, feature_cols, n_months=3, top_k=30):\n",
    "    \"\"\"\n",
    "    D·ª± ƒëo√°n top-K destinations cho n_months th√°ng t·ªõi\n",
    "    \n",
    "    C√°ch ho·∫°t ƒë·ªông:\n",
    "    1. L·∫•y d·ªØ li·ªáu c·ªßa th√°ng hi·ªán t·∫°i\n",
    "    2. T·∫°o features cho th√°ng ti·∫øp theo\n",
    "    3. D·ª± ƒëo√°n traffic\n",
    "    4. Ranking v√† tr·∫£ v·ªÅ top-K\n",
    "    \"\"\"\n",
    "    \n",
    "    # L·∫•y th√°ng cu·ªëi c√πng trong dataset\n",
    "    latest_date = df['date_parsed'].max()\n",
    "    latest_month = latest_date.month\n",
    "    latest_year = latest_date.year\n",
    "    \n",
    "    print(f\"üìÖ D·ªØ li·ªáu m·ªõi nh·∫•t: {latest_date.strftime('%Y-%m')}\")\n",
    "    print(f\"üìÖ D·ª± b√°o cho {n_months} th√°ng t·ªõi:\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(1, n_months + 1):\n",
    "        # T√≠nh th√°ng c·∫ßn d·ª± b√°o\n",
    "        forecast_month = (latest_month + i - 1) % 12 + 1\n",
    "        forecast_year = latest_year + (latest_month + i - 1) // 12\n",
    "        \n",
    "        print(f\"üîÆ Th√°ng {forecast_month}/{forecast_year}:\")\n",
    "        \n",
    "        # L·∫•y d·ªØ li·ªáu c·ªßa th√°ng n√†y t·ª´ c√°c nƒÉm tr∆∞·ªõc ƒë·ªÉ t·∫°o features\n",
    "        # (trong th·ª±c t·∫ø s·∫Ω c·∫ßn update lag features ph·ª©c t·∫°p h∆°n)\n",
    "        month_data = df[df['month'] == forecast_month].copy()\n",
    "        \n",
    "        if len(month_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # D·ª± ƒëo√°n\n",
    "        X = month_data[feature_cols].fillna(0)\n",
    "        month_data['predicted_traffic'] = model.predict(X)\n",
    "        \n",
    "        # Ranking\n",
    "        month_data['rank'] = month_data['predicted_traffic'].rank(ascending=False)\n",
    "        top_destinations = month_data.nlargest(top_k, 'predicted_traffic')[\n",
    "            ['destination', 'province', 'predicted_traffic', 'has_strong_seasonality', 'is_peak_month']\n",
    "        ].reset_index(drop=True)\n",
    "        \n",
    "        top_destinations['forecast_month'] = forecast_month\n",
    "        top_destinations['forecast_year'] = forecast_year\n",
    "        results.append(top_destinations)\n",
    "        \n",
    "        print(f\"   Top 5: {', '.join(top_destinations['destination'].head(5).tolist())}\")\n",
    "        print()\n",
    "    \n",
    "    return pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
    "\n",
    "# Ch·∫°y forecast\n",
    "forecast_results = forecast_upcoming_months(model_lgb, df_features, FEATURE_COLS, n_months=3, top_k=30)\n",
    "print(f\"\\n‚úÖ ƒê√£ d·ª± b√°o {len(forecast_results)} recommendations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# B∆Ø·ªöC 9.3: HI·ªÇN TH·ªä K·∫æT QU·∫¢ D·ª∞ B√ÅO\n",
    "# =============================================================================\n",
    "\n",
    "# L·∫•y th√°ng d·ª± b√°o\n",
    "if len(forecast_results) > 0:\n",
    "    forecast_results['month_year'] = forecast_results.apply(\n",
    "        lambda x: f\"Th√°ng {int(x['forecast_month'])}/{int(x['forecast_year'])}\", axis=1\n",
    "    )\n",
    "    \n",
    "    # Pivot ƒë·ªÉ hi·ªÉn th·ªã top destinations m·ªói th√°ng\n",
    "    print(\"=\"*80)\n",
    "    print(\"üèÜ TOP ƒêI·ªÇM ƒê·∫æN D·ª∞ B√ÅO C√ì TRAFFIC CAO NH·∫§T\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for month_year in forecast_results['month_year'].unique():\n",
    "        month_data = forecast_results[forecast_results['month_year'] == month_year].head(15)\n",
    "        \n",
    "        print(f\"\\nüìÖ {month_year}:\")\n",
    "        print(\"-\"*60)\n",
    "        for i, row in month_data.iterrows():\n",
    "            peak_flag = \"üî•\" if row['is_peak_month'] == 1 else \"\"\n",
    "            seasonal_flag = \"üìà\" if row['has_strong_seasonality'] else \"\"\n",
    "            print(f\"   {month_data.index.get_loc(i)+1:2}. {row['destination']:<35} ({row['province']}) {peak_flag}{seasonal_flag}\")\n",
    "            print(f\"       Predicted traffic: {row['predicted_traffic']:.1f}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng c√≥ k·∫øt qu·∫£ d·ª± b√°o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34fcc7",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã B∆Ø·ªöC 10: T√ìM T·∫ÆT V√Ä ƒê·ªÄ XU·∫§T TI·∫æP THEO\n",
    "\n",
    "### ‚úÖ ƒê√£ ho√†n th√†nh:\n",
    "1. **Data Exploration**: Ph√¢n t√≠ch 174,060 records, 967 destinations, 15 nƒÉm d·ªØ li·ªáu\n",
    "2. **Feature Engineering**: T·∫°o 35+ features t·ª´ lag, rolling, seasonality, weather, infrastructure\n",
    "3. **Modeling**: LightGBM v·ªõi early stopping, ƒë·∫°t c·∫£i thi·ªán MAE so v·ªõi baseline\n",
    "4. **Prediction**: D·ª± b√°o top destinations cho 1-3 th√°ng t·ªõi\n",
    "5. **Ranking Evaluation**: Precision@K, Recall@K metrics\n",
    "\n",
    "### üöÄ C·∫£i ti·∫øn ti·∫øp theo:\n",
    "\n",
    "| ∆Øu ti√™n | Task | Chi ti·∫øt |\n",
    "|---------|------|----------|\n",
    "| üî¥ **Cao** | Hyperparameter tuning | GridSearch/Optuna cho LightGBM |\n",
    "| üî¥ **Cao** | Cross-validation | TimeSeriesSplit v·ªõi multiple folds |\n",
    "| üü° **Trung b√¨nh** | Th√™m features | L·ªÖ h·ªôi (festival calendar), events ƒë·∫∑c bi·ªát |\n",
    "| üü° **Trung b√¨nh** | Prophet model | Cho top destinations c√≥ strong seasonality |\n",
    "| üü¢ **Th·∫•p** | Neural Network | LSTM/Transformer cho sequence modeling |\n",
    "| üü¢ **Th·∫•p** | API/Dashboard | Productionize predictions |\n",
    "\n",
    "### üí° Insights quan tr·ªçng:\n",
    "1. **Lag features** (ƒë·∫∑c bi·ªát lag_12m - c√πng th√°ng nƒÉm tr∆∞·ªõc) l√† quan tr·ªçng nh·∫•t\n",
    "2. **Destinations c√≥ strong seasonality** d·ªÖ d·ª± ƒëo√°n h∆°n\n",
    "3. **Weather v√† infrastructure** c√≥ ·∫£nh h∆∞·ªüng nh∆∞ng kh√¥ng m·∫°nh b·∫±ng historical patterns\n",
    "4. **Zero-inflation** c·∫ßn ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·∫∑c bi·ªát (nhi·ªÅu destinations c√≥ 0 traffic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcd64f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BONUS: EXPORT K·∫æT QU·∫¢\n",
    "# =============================================================================\n",
    "\n",
    "# Save trained model\n",
    "import joblib\n",
    "\n",
    "# L∆∞u model\n",
    "joblib.dump(model_lgb, '../models/lgb_traffic_predictor.pkl')\n",
    "print(\"‚úÖ Model saved to ../models/lgb_traffic_predictor.pkl\")\n",
    "\n",
    "# L∆∞u predictions\n",
    "if len(forecast_results) > 0:\n",
    "    forecast_results.to_csv('../data/predictions/upcoming_top_destinations.csv', index=False)\n",
    "    print(\"‚úÖ Predictions saved to ../data/predictions/upcoming_top_destinations.csv\")\n",
    "\n",
    "# L∆∞u feature importance\n",
    "importance_df.to_csv('../data/predictions/feature_importance.csv', index=False)\n",
    "print(\"‚úÖ Feature importance saved to ../data/predictions/feature_importance.csv\")\n",
    "\n",
    "print(\"\\nüéâ HO√ÄN TH√ÄNH PH√ÇN T√çCH!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
